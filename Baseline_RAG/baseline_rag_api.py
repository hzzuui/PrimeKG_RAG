import json
import requests
import os
import csv
from collections import defaultdict
from sentence_transformers import SentenceTransformer
from pymilvus import connections, Collection


# ========== åƒæ•¸è¨­å®š ==========
MILVUS_COLLECTION = "collection_text"
QUERY_FILE = "data/primekg_queries_multihop.jsonl"
OLLAMA_API_URL = "http://localhost:11434/v1/chat/completions"
LLM_MODEL = "gpt-oss:20b"  

RESULT_DIR = "results"
RESULT_JSON = os.path.join(RESULT_DIR, "baseline_result.json")
RESULT_CSV  = os.path.join(RESULT_DIR, "baseline_result.csv")

# ========== åˆå§‹åŒ– ==========
print("ğŸ”§ é€£ç·šåˆ° Milvus ...")
connections.connect(
    alias="default",
    host="127.0.0.1",   # å¦‚æœæ˜¯ Docker èµ·çš„ Milvusï¼Œé€™è£¡è¦æ”¹æˆå®¹å™¨å°å¤–çš„ IP
    port="19530"
)

print("ğŸ”§ è¼‰å…¥æ¨¡å‹èˆ‡ Milvus ...")
embedder = SentenceTransformer("all-mpnet-base-v2") # embedding model
collection = Collection(MILVUS_COLLECTION)
# å¦‚æœé‚„æ²’æœ‰ indexï¼Œå»ºç«‹ä¸€å€‹
index_params = {
    "index_type": "IVF_FLAT",   # ä½ ä¹Ÿå¯ä»¥æ”¹æˆ HNSW / AUTOINDEX
    "metric_type": "IP",        # IP = inner product (cosine ç›¸ä¼¼åº¦)
    "params": {"nlist": 128}
}
try:
    collection.create_index(field_name="embedding", index_params=index_params)
    print("âœ… å·²å»ºç«‹ç´¢å¼• (embedding)")
except Exception as e:
    print(f"âš ï¸ å»ºç«‹ç´¢å¼•æ™‚è·³é: {e}")

# å†è¼‰å…¥ collection
collection.load()
print(f"âœ… Collection {MILVUS_COLLECTION} å·²è¼‰å…¥")

# ========== å·¥å…·å‡½å¼ ==========
# åœ¨æŒ‡å®šçš„ Milvus collection è£¡æª¢ç´¢ç›¸ä¼¼å‘é‡ã€‚
def search_milvus(query_text, top_k=5):
    """å°‡ query å‘é‡åŒ–ï¼Œä¸¦åœ¨ Milvus æª¢ç´¢æœ€ç›¸ä¼¼çš„ç‰‡æ®µ"""
    q_emb = embedder.encode(query_text).tolist()
    search_res = collection.search(
        data=[q_emb],
        anns_field="embedding", # æœå°‹çš„åŸºç¤æ˜¯ embedding æ¬„ä½ã€‚
        param={"metric_type": "IP", "nprobe": 10}, 
        limit=top_k,
        output_fields=["source_name", "relation_type", "target_name"]
    )
    # æŠŠåŸæœ¬ä¸‰å€‹åˆ†é–‹çš„æ¬„ä½ â†’ åˆä½µæˆä¸€æ®µæ–‡å­—ï¼Œæä¾›çµ¦ LLM ç•¶ contextã€‚
    hits = [f"{hit.entity.get('source_name')} {hit.entity.get('relation_type')} {hit.entity.get('target_name')}"  # æŒ‡å®šè¦å–å‡ºçš„æ¬„ä½ã€‚
        for hit in search_res[0]]

    return hits # å›å‚³æª¢ç´¢åˆ°çš„æ–‡å­—ç‰‡æ®µæ¸…å–®

def ask_llm(query_text, context, model=LLM_MODEL):
    """ç”¨ requests å‘¼å« Ollama API"""
    prompt = f"""
ä½ æ˜¯ä¸€ä½ç”Ÿé†«çŸ¥è­˜å°ˆå®¶ï¼Œä»¥ä¸‹æ˜¯çŸ¥è­˜åº«æª¢ç´¢åˆ°çš„ç›¸é—œè³‡æ–™ï¼š
{chr(10).join('- ' + c for c in context)}

å•é¡Œï¼š{query_text}
è«‹ç›´æ¥åˆ—å‡ºåŸºå› æ¸…å–®ï¼Œç”¨é€—è™Ÿåˆ†éš”ã€‚
"""
    response = requests.post(
        OLLAMA_API_URL,
        headers={"Content-Type": "application/json"},
        json={
            "model": model,
            "messages": [{"role": "user", "content": prompt}]
        }
    )

    if response.status_code != 200:
        raise RuntimeError(f"LLM API call failed: {response.status_code}, {response.text}")

    data = response.json()
    return data["choices"][0]["message"]["content"]

def compute_metrics(pred, gold):
    """è¨ˆç®— Precision / Recall / F1"""
    pred_set, gold_set = set(pred), set(gold)
    tp = len(pred_set & gold_set)
    fp = len(pred_set - gold_set)
    fn = len(gold_set - pred_set)
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2*precision*recall/(precision+recall) if (precision+recall) > 0 else 0
    return precision, recall, f1

# ========== ä¸»ç¨‹å¼ ==========
if __name__ == "__main__":
    print("ğŸš€ é–‹å§‹ baseline RAG æ¸¬è©¦ ...")
    gold_queries = [json.loads(line) for line in open(QUERY_FILE, "r", encoding="utf-8")]
    all_results = []
    all_p, all_r, all_f1 = [], [], []

    for q in gold_queries : 
        if q.get("type") != "multi-hop":
            continue
        question = q["question"]
        gold = [g.upper() for g in q["gold_answers"]]

        print(f"\nğŸ” å•é¡Œ: {question}")

        # Step 1: Milvus æª¢ç´¢
        context = search_milvus(question, top_k=5)
        print(f"  æª¢ç´¢åˆ° {len(context)} å€‹ç‰‡æ®µ")

        # Step 2: LLM å›ç­”
        answer = ask_llm(question, context)
        print(f"  LLM å›ç­”: {answer}")

        # Step 3: æ ¼å¼åŒ– LLM å›ç­”ï¼ˆé€—è™Ÿåˆ†éš” â†’ listï¼‰
        pred = [a.strip().upper() for a in answer.split(",") if a.strip()]

        # Step 4: è¨ˆç®—æŒ‡æ¨™
        p, r, f1 = compute_metrics(pred, gold)
        print(f"  Precision={p:.3f}, Recall={r:.3f}, F1={f1:.3f}")

        # Step 5: å­˜çµæœ
        result = {
            "question": question,
            "gold_answers": gold,
            "retrieved_context": context,
            "llm_answer": answer,
            "pred_genes": pred,
            "precision": p,
            "recall": r,
            "f1": f1
        }
        all_results.append(result)

        

    # === è¼¸å‡º JSON ===
    with open(RESULT_JSON, "w", encoding="utf-8") as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)

    # === è¼¸å‡º CSV ===
    with open(RESULT_CSV, "w", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=list(all_results[0].keys()))
        writer.writeheader()
        writer.writerows(all_results)

        # ç¸½é«”å¹³å‡
        avg_precision = sum(r["precision"] for r in all_results) / len(all_results)
        avg_recall = sum(r["recall"] for r in all_results) / len(all_results)
        avg_f1 = sum(r["f1"] for r in all_results) / len(all_results)

        writer.writerow({
            "question": "AVERAGE",
            "gold_answers": "-",
            "retrieved_context": "-",
            "llm_answer": "-",
            "pred_genes": "-",
            "precision": avg_precision,
            "recall": avg_recall,
            "f1": avg_f1
        })

    print(f"\nğŸ’¾ å·²å­˜æª”: {RESULT_JSON}, {RESULT_CSV}")