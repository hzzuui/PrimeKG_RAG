import json
import requests
import os, sys , re
import csv
import pandas as pd
from collections import defaultdict
from sentence_transformers import SentenceTransformer
from pymilvus import connections, Collection

# === åŒ¯å…¥ Neo4j é€£ç·š & PAG ç”Ÿæˆå™¨ ===
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), ".")))
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from backend.neo4j_connect import Neo4jConnection
from pag_generator import generate_pag,generate_pag_drug,generate_pag_with_genes
from apriori.rule_generator import mine_rules_from_json
# ========== åƒæ•¸è¨­å®š ==========
MILVUS_COLLECTION = "collection_text"
QUERY_FILE = "data/primekg_queries_multihop.jsonl"
OLLAMA_API_URL = "http://localhost:11434/v1/chat/completions"
LLM_MODEL = "gpt-oss:20b"  

RESULT_DIR = "results"
RESULT_JSON = os.path.join(RESULT_DIR, "enhanced_apriori_graph_result.json")
RESULT_CSV  = os.path.join(RESULT_DIR, "enhanced_apriori_graph_result.csv")

# ========== è¼‰å…¥ Apriori è¦å‰‡åº« ==========
RULE_BASE_PATH = "data/apriori_rule_base.csv"
if os.path.exists(RULE_BASE_PATH):
    rule_base = pd.read_csv(RULE_BASE_PATH)
    print(f"âœ… å·²è¼‰å…¥ {len(rule_base)} æ¢ Apriori è¦å‰‡")
else:
    rule_base = pd.DataFrame()
    print("âš ï¸ å°šæœªå»ºç«‹ Apriori è¦å‰‡åº« (data/apriori_rule_base.csv)")


# ========== åˆå§‹åŒ– ==========
print("ğŸ”§ é€£ç·šåˆ° Milvus ...")
connections.connect(alias="default", host="127.0.0.1", port="19530")

print("ğŸ”§ è¼‰å…¥æ¨¡å‹èˆ‡ Milvus ...")
embedder = SentenceTransformer("all-mpnet-base-v2") # embedding model
collection = Collection(MILVUS_COLLECTION)

# å»ºç«‹ç´¢å¼•
index_params = {
    "index_type": "IVF_FLAT",
    "metric_type": "IP",  # IP = cosine similarity
    "params": {"nlist": 128}
}
try:
    collection.create_index(field_name="embedding", index_params=index_params)
    print("âœ… å·²å»ºç«‹ç´¢å¼• (embedding)")
except Exception as e:
    print(f"âš ï¸ å»ºç«‹ç´¢å¼•æ™‚è·³é: {e}")

collection.load()
print(f"âœ… Collection {MILVUS_COLLECTION} å·²è¼‰å…¥")


# ========== è¼‰å…¥ Apriori è¦å‰‡åº« ==========
RULE_BASE_PATH = "data/apriori_rule_base.csv"
if os.path.exists(RULE_BASE_PATH):
    rule_base = pd.read_csv(RULE_BASE_PATH)
    print(f"âœ… å·²è¼‰å…¥ {len(rule_base)} æ¢ Apriori è¦å‰‡")
else:
    rule_base = pd.DataFrame()
    print("âš ï¸ å°šæœªå»ºç«‹ Apriori è¦å‰‡åº« (data/apriori_rule_base.csv)")


# ========== å·¥å…·å‡½å¼ ==========

def search_milvus(query_text, top_k=5):
    """å°‡ query å‘é‡åŒ–ï¼Œåœ¨ Milvus æª¢ç´¢ç›¸ä¼¼ç‰‡æ®µ"""
    q_emb = embedder.encode(query_text).tolist()
    search_res = collection.search(
        data=[q_emb],
        anns_field="embedding",
        param={"metric_type": "IP", "nprobe": 10}, 
        limit=top_k,
        output_fields=["source_name", "relation_type", "target_name"]
    )
    hits = [f"{hit.entity.get('source_name')} {hit.entity.get('relation_type')} {hit.entity.get('target_name')}"
            for hit in search_res[0]]
    return hits

def build_drug_gene_context(milvus_hits, conn: Neo4jConnection, limit=5):
    """Neo4j æ“´å±•ï¼šç–¾ç—… â†’ è—¥ç‰© â†’ åŸºå› """
    context = []
    for hit in milvus_hits:
        entity = hit.split(" ")[0] if isinstance(hit, str) else hit.get("source_name", "")
        if not entity:
            continue
        query = f"""
        MATCH (d:disease)<-[:indication]-(drug:drug)-[:drug_protein]-(g:gene__protein)
        WHERE toLower(d.node_name) CONTAINS toLower("{entity}")
        RETURN d.node_name AS disease, drug.node_name AS drug, g.node_name AS gene
        LIMIT {limit}
        """
        records = conn.query(query)
        for r in records:
            context.append(f"{r['disease']} indication {r['drug']} drug_protein {r['gene']}")
    return context

def build_pag_context(milvus_hits, conn: Neo4jConnection, limit=2):
    """æ ¹æ“šæª¢ç´¢çš„ç–¾ç—…ï¼Œç”Ÿæˆ PAG sentences"""
    pag_context = []
    for hit in milvus_hits:
        entity = hit.split(" ")[0] if isinstance(hit, str) else hit.get("source_name", "")
        if not entity:
            continue
        try:
            pag_results = generate_pag(entity, limit=limit)
            for pag in pag_results:
                pag_context.append(pag["pag_sentence"])
        except Exception as e:
            print(f"âš ï¸ PAG ç”Ÿæˆå¤±æ•—: {e}")
    return pag_context


def extract_entity_from_query(query_text: str):
    """ç°¡å–®è¦å‰‡å¼æŠ½å–å¯¦é«”åç¨±ï¼ˆæœªä¾†å¯æ¥ NERï¼‰"""
    match = re.search(r"æ²»ç™‚(.+?)çš„è—¥ç‰©", query_text)
    if match:
        return match.group(1).strip()
    return None

def generate_dynamic_rules(query_text):
    """æ ¹æ“š query å…§å®¹å‹•æ…‹é¸æ“‡è¦æŒ–æ˜çš„é—œä¿‚"""
    if "æ²»ç™‚" in query_text or "disease" in query_text.lower():
        relation = "disease_protein"
    elif "mechanism" in query_text or "pathway" in query_text.lower():
        relation = "bioprocess_protein"
    else:
        relation = "protein_protein"

    entity_keyword = extract_entity_from_query(query_text)
    if not entity_keyword:
        print("âš ï¸ æœªèƒ½å¾ query ä¸­æŠ½å‡ºå¯¦é«”ï¼Œç•¥é Apriori æŒ–æ˜")
        return []

    print(f"ğŸ” å‹•æ…‹æŒ–æ˜è¦å‰‡: relation={relation}, keyword={entity_keyword}")
    rules = mine_rules_from_json(
        triple_path="data/neo4j_triples.json",
        relation_filter=relation,
        entity_keyword=entity_keyword,
        min_support=0.005,
        min_conf=0.7,
        output_path="data/temp_rule_base.csv"
    )

    if rules.empty:
        return []
    
    context = [
        f"If {', '.join(r['antecedents'])} occurs, {', '.join(r['consequents'])} is likely (conf={r['confidence']:.2f})"
        for _, r in rules.iterrows()
    ]
    return context

def ask_llm(query_text, context, model=LLM_MODEL):
    """å‘¼å« Ollama API"""
    prompt = f"""
ä½ æ˜¯ä¸€ä½ç”Ÿé†«çŸ¥è­˜å°ˆå®¶ï¼Œä»¥ä¸‹æ˜¯çŸ¥è­˜åº«æª¢ç´¢åˆ°çš„ç›¸é—œè³‡æ–™ï¼š
{chr(10).join('- ' + c for c in context)}

å•é¡Œï¼š{query_text}
è«‹ç›´æ¥åˆ—å‡ºåŸºå› æ¸…å–®ï¼Œç”¨é€—è™Ÿåˆ†éš”ã€‚
"""
    response = requests.post(
        OLLAMA_API_URL,
        headers={"Content-Type": "application/json"},
        json={
            "model": model,
            "messages": [{"role": "user", "content": prompt}]
        }
    )
    if response.status_code != 200:
        raise RuntimeError(f"LLM API call failed: {response.status_code}, {response.text}")
    data = response.json()
    return data["choices"][0]["message"]["content"]

def compute_metrics(pred, gold):
    """Precision / Recall / F1"""
    pred_set, gold_set = set(pred), set(gold)
    tp = len(pred_set & gold_set)
    fp = len(pred_set - gold_set)
    fn = len(gold_set - pred_set)
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2*precision*recall/(precision+recall) if (precision+recall) > 0 else 0
    return precision, recall, f1

# ========== ä¸»ç¨‹å¼ ==========
if __name__ == "__main__":
    print("ğŸš€ é–‹å§‹ Enhanced-RAG æ¸¬è©¦ ...")
    gold_queries = [json.loads(line) for line in open(QUERY_FILE, "r", encoding="utf-8")]
    all_results = []
    all_p, all_r, all_f1 = [], [], []

    conn = Neo4jConnection()

    for q in gold_queries:
        if q.get("type") != "multi-hop":
            continue
        question = q["question"]
        gold = [g.upper() for g in q["gold_answers"]]

        print(f"\nğŸ” å•é¡Œ: {question}")

        # Step 1: Milvus æª¢ç´¢
        milvus_context = search_milvus(question, top_k=5)
        print(f"  æª¢ç´¢åˆ° {len(milvus_context)} å€‹ç‰‡æ®µ")

        # Step 2: Neo4j æ“´å±•
        graph_context = build_drug_gene_context(milvus_context, conn)
        print(f"  Neo4j æ“´å±•åˆ° {len(graph_context)} å€‹ç‰‡æ®µ")

        # Step 3: PAG ç”Ÿæˆ
        # pag_context = build_pag_context(milvus_context, conn)
        pag_context = []
        for hit in milvus_context:
            entity = hit.split(" ")[0] if isinstance(hit, str) else hit.get("source_name", "")
            if not entity:
                continue

            # diseaseâ€“drug å±¤ç´š
            pag_drug = generate_pag_drug(entity, conn, limit=3)
            # diseaseâ€“drugâ€“gene å±¤ç´š
            pag_gene = generate_pag_with_genes(entity, conn, limit=3)

            # æŠŠå¥å­æŠ½å‡ºä¾†
            for pag in (pag_drug + pag_gene):
                pag_context.append(pag["pag_sentence"])

        print(f"  PAG ç”Ÿæˆ {len(pag_context)} å€‹å¥å­")


        # Step 3.5: Apriori-based reasoning
        apriori_context = generate_dynamic_rules(question)
        print(f"  ğŸ”§ å‹•æ…‹ Apriori è¦å‰‡æŒ–æ˜çµæœ: {len(apriori_context)} æ¢")

        # Step 4: åˆä½µ context
        full_context = milvus_context + graph_context + pag_context + apriori_context

        # Step 5: LLM å›ç­”
        answer = ask_llm(question, full_context)

        # Step 6: æ ¼å¼åŒ–å›ç­”
        pred = [a.strip().upper() for a in answer.split(",") if a.strip()]

        # Step 7: è¨ˆç®—æŒ‡æ¨™
        p, r, f1 = compute_metrics(pred, gold)
        print(f"  Precision={p:.3f}, Recall={r:.3f}, F1={f1:.3f}")

        # Step 8: å­˜çµæœ
        result = {
            "question": question,
            "gold_answers": gold,
            "retrieved_context": full_context,
            "llm_answer": answer,
            "pred_genes": pred,
            "precision": p,
            "recall": r,
            "f1": f1
        }
        all_results.append(result)

    # === è¼¸å‡º JSON ===
    with open(RESULT_JSON, "w", encoding="utf-8") as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)

    # === è¼¸å‡º CSV ===
    with open(RESULT_CSV, "w", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=list(all_results[0].keys()))
        writer.writeheader()
        writer.writerows(all_results)

    # ç¸½é«”å¹³å‡
        avg_precision = sum(r["precision"] for r in all_results) / len(all_results)
        avg_recall = sum(r["recall"] for r in all_results) / len(all_results)
        avg_f1 = sum(r["f1"] for r in all_results) / len(all_results)

        writer.writerow({
            "question": "AVERAGE",
            "gold_answers": "-",
            "retrieved_context": "-",
            "llm_answer": "-",
            "pred_genes": "-",
            "precision": avg_precision,
            "recall": avg_recall,
            "f1": avg_f1
        })

    print(f"\nğŸ’¾ å·²å­˜æª”: {RESULT_JSON}, {RESULT_CSV}")
