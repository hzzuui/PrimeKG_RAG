import json
import requests
import os
import csv
import sys
from collections import defaultdict
from sentence_transformers import SentenceTransformer
from pymilvus import connections, Collection
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from backend.neo4j_connect import Neo4jConnection

# ========== åƒæ•¸è¨­å®š ==========
MILVUS_COLLECTION = "collection_text"
QUERY_FILE = "data/primekg_queries_multihop.jsonl"
OLLAMA_API_URL = "http://localhost:11434/v1/chat/completions"
LLM_MODEL = "gpt-oss:20b"  

RESULT_DIR = "results"
RESULT_JSON = os.path.join(RESULT_DIR, "graph_result.json")
RESULT_CSV  = os.path.join(RESULT_DIR, "graph_result.csv")

# ========== åˆå§‹åŒ– ==========
print("ğŸ”§ é€£ç·šåˆ° Milvus ...")
connections.connect(
    alias="default",
    host="127.0.0.1",   # å¦‚æœæ˜¯ Docker èµ·çš„ Milvusï¼Œé€™è£¡è¦æ”¹æˆå®¹å™¨å°å¤–çš„ IP
    port="19530"
)

print("ğŸ”§ è¼‰å…¥æ¨¡å‹èˆ‡ Milvus ...")
embedder = SentenceTransformer("all-mpnet-base-v2") # embedding model
collection = Collection(MILVUS_COLLECTION)
# å¦‚æœé‚„æ²’æœ‰ indexï¼Œå»ºç«‹ä¸€å€‹
index_params = {
    "index_type": "IVF_FLAT",   # ä½ ä¹Ÿå¯ä»¥æ”¹æˆ HNSW / AUTOINDEX
    "metric_type": "IP",        # IP = inner product (cosine ç›¸ä¼¼åº¦)
    "params": {"nlist": 128}
}
try:
    collection.create_index(field_name="embedding", index_params=index_params)
    print("âœ… å·²å»ºç«‹ç´¢å¼• (embedding)")
except Exception as e:
    print(f"âš ï¸ å»ºç«‹ç´¢å¼•æ™‚è·³é: {e}")

# å†è¼‰å…¥ collection
collection.load()
print(f"âœ… Collection {MILVUS_COLLECTION} å·²è¼‰å…¥")

# ========== å·¥å…·å‡½å¼ ==========
# åœ¨æŒ‡å®šçš„ Milvus collection è£¡æª¢ç´¢ç›¸ä¼¼å‘é‡ã€‚
def search_milvus(query_text, top_k=5):
    """å°‡ query å‘é‡åŒ–ï¼Œä¸¦åœ¨ Milvus æª¢ç´¢æœ€ç›¸ä¼¼çš„ç‰‡æ®µ"""
    q_emb = embedder.encode(query_text).tolist()
    search_res = collection.search(
        data=[q_emb],
        anns_field="embedding", # æœå°‹çš„åŸºç¤æ˜¯ embedding æ¬„ä½ã€‚
        param={"metric_type": "IP", "nprobe": 10}, 
        limit=top_k,
        output_fields=["source_name", "relation_type", "target_name"]
    )
    # æŠŠåŸæœ¬ä¸‰å€‹åˆ†é–‹çš„æ¬„ä½ â†’ åˆä½µæˆä¸€æ®µæ–‡å­—ï¼Œæä¾›çµ¦ LLM ç•¶ contextã€‚
    hits = [f"{hit.entity.get('source_name')} {hit.entity.get('relation_type')} {hit.entity.get('target_name')}"  # æŒ‡å®šè¦å–å‡ºçš„æ¬„ä½ã€‚
        for hit in search_res[0]]

    return hits # å›å‚³æª¢ç´¢åˆ°çš„æ–‡å­—ç‰‡æ®µæ¸…å–®


def expand_with_neo4j(entity_name: str, conn: Neo4jConnection, hops=2, limit=5):
    query = f"""
    MATCH path = (n {{node_name: "{entity_name}"}})-[*1..{hops}]-(m)
    RETURN path LIMIT {limit}
    """
    return conn.query(query)

def build_graph_context(milvus_hits, conn: Neo4jConnection):
    context = []
    for hit in milvus_hits:
        ...
        query = f"""
        MATCH path = (n)-[*1..2]-(m)
        WHERE toLower(n.node_name) CONTAINS toLower("{entity}")
        RETURN path LIMIT 5
        """
        paths = conn.query(query)
        ...


# def build_drug_gene_context(entity_name: str, conn: Neo4jConnection, limit=5):
#     query = f"""
#     MATCH (d:disease)<-[:indication]-(drug:drug)-[:drug_protein]-(g:gene__protein)
#     WHERE toLower(d.node_name) CONTAINS toLower("{entity_name}")
#     RETURN d.node_name AS disease, drug.node_name AS drug, g.node_name AS gene
#     LIMIT {limit}
#     """
#     records = conn.query(query)
#     return [f"{r['disease']} indication {r['drug']} drug_protein {r['gene']}" for r in records]


def build_drug_gene_context(milvus_hits, conn: Neo4jConnection, limit=5):
    """
    æ ¹æ“š Milvus æª¢ç´¢çµæœ (ç–¾ç—…ç›¸é—œç¯€é»)ï¼Œåˆ° Neo4j æŸ¥è©¢ç–¾ç—…-è—¥ç‰©-åŸºå› è·¯å¾‘
    """
    context = []

    for hit in milvus_hits:
        # å…¼å®¹ string å’Œ dict æ ¼å¼
        if isinstance(hit, str):
            entity = hit.split(" ")[0]  # å‡è¨­ç¬¬ä¸€å€‹è©æ˜¯ç–¾ç—…åç¨±
        elif isinstance(hit, dict):
            entity = hit.get("source_name", "")
        else:
            continue

        if not entity:
            continue

        # æŸ¥è©¢ Neo4j (disease-drug-gene)
        query = f"""
        MATCH (d:disease)<-[:indication]-(drug:drug)-[:drug_protein]-(g:gene__protein)
        WHERE toLower(d.node_name) CONTAINS toLower("{entity}")
        RETURN d.node_name AS disease, drug.node_name AS drug, g.node_name AS gene
        LIMIT {limit}
        """
        records = conn.query(query)

        # æŠŠçµæœè½‰æˆå­—ä¸²ï¼Œæ–¹ä¾¿çµ¦ LLM ç•¶ context
        for r in records:
            context.append(f"{r['disease']} indication {r['drug']} drug_protein {r['gene']}")

    return context


def ask_llm(query_text, context, model=LLM_MODEL):
    """ç”¨ requests å‘¼å« Ollama API"""
    prompt = f"""
ä½ æ˜¯ä¸€ä½ç”Ÿé†«çŸ¥è­˜å°ˆå®¶ï¼Œä»¥ä¸‹æ˜¯çŸ¥è­˜åº«æª¢ç´¢åˆ°çš„ç›¸é—œè³‡æ–™ï¼š
{chr(10).join('- ' + c for c in context)}

å•é¡Œï¼š{query_text}
è«‹ç›´æ¥åˆ—å‡ºåŸºå› æ¸…å–®ï¼Œç”¨é€—è™Ÿåˆ†éš”ã€‚
"""
    response = requests.post(
        OLLAMA_API_URL,
        headers={"Content-Type": "application/json"},
        json={
            "model": model,
            "messages": [{"role": "user", "content": prompt}]
        }
    )

    if response.status_code != 200:
        raise RuntimeError(f"LLM API call failed: {response.status_code}, {response.text}")

    data = response.json()
    return data["choices"][0]["message"]["content"]



def compute_metrics(pred, gold):
    """è¨ˆç®— Precision / Recall / F1"""
    pred_set, gold_set = set(pred), set(gold)
    tp = len(pred_set & gold_set)
    fp = len(pred_set - gold_set)
    fn = len(gold_set - pred_set)
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2*precision*recall/(precision+recall) if (precision+recall) > 0 else 0
    return precision, recall, f1

# ========== ä¸»ç¨‹å¼ ==========
if __name__ == "__main__":
    print("ğŸš€ é–‹å§‹ Graph RAG æ¸¬è©¦ ...")
    gold_queries = [json.loads(line) for line in open(QUERY_FILE, "r", encoding="utf-8")]
    all_results = []
    all_p, all_r, all_f1 = [], [], []

    conn = Neo4jConnection()

    for q in gold_queries : 
        if q.get("type") != "multi-hop":
            continue
        question = q["question"]
        gold = [g.upper() for g in q["gold_answers"]]

        print(f"\nğŸ” å•é¡Œ: {question}")

        
        # Step 1: Milvus æª¢ç´¢
        milvus_context = search_milvus(question, top_k=5)
        print(f"  æª¢ç´¢åˆ° {len(milvus_context)} å€‹ç‰‡æ®µ")

       # Step 2: Neo4j æ“´å±•
        graph_context = build_drug_gene_context(milvus_context, conn)
        print(f"  Neo4j æ“´å±•åˆ° {len(graph_context)} å€‹ç‰‡æ®µ")

        # Step 3: åˆä½µ context
        full_context = milvus_context + graph_context

        # Step 4: LLM å›ç­”
        answer = ask_llm(question, full_context)

        # Step 5: æ ¼å¼åŒ– LLM å›ç­”ï¼ˆé€—è™Ÿåˆ†éš” â†’ listï¼‰
        pred = [a.strip().upper() for a in answer.split(",") if a.strip()]

        # Step 4: è¨ˆç®—æŒ‡æ¨™
        p, r, f1 = compute_metrics(pred, gold)
        print(f"  Precision={p:.3f}, Recall={r:.3f}, F1={f1:.3f}")

        # Step 5: å­˜çµæœ
        result = {
            "question": question,
            "gold_answers": gold,
            "retrieved_context": full_context,
            "llm_answer": answer,
            "pred_genes": pred,
            "precision": p,
            "recall": r,
            "f1": f1
        }
        all_results.append(result)

        

    # === è¼¸å‡º JSON ===
    with open(RESULT_JSON, "w", encoding="utf-8") as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)

    # === è¼¸å‡º CSV ===
    with open(RESULT_CSV, "w", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=list(all_results[0].keys()))
        writer.writeheader()
        writer.writerows(all_results)

        # ç¸½é«”å¹³å‡
        avg_precision = sum(r["precision"] for r in all_results) / len(all_results)
        avg_recall = sum(r["recall"] for r in all_results) / len(all_results)
        avg_f1 = sum(r["f1"] for r in all_results) / len(all_results)

        writer.writerow({
            "question": "AVERAGE",
            "gold_answers": "-",
            "retrieved_context": "-",
            "llm_answer": "-",
            "pred_genes": "-",
            "precision": avg_precision,
            "recall": avg_recall,
            "f1": avg_f1
        })

    print(f"\nğŸ’¾ å·²å­˜æª”: {RESULT_JSON}, {RESULT_CSV}")