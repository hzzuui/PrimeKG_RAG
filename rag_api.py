import time
from flask import Flask, request, jsonify
from flask import Response, stream_with_context
from pymilvus import connections, Collection, utility
from sentence_transformers import SentenceTransformer
from langchain_ollama import OllamaLLM
from flask import Response
import json
import warnings
import sys
import socket
from flask_cors import CORS
from backend.neo4j_connect import Neo4jConnection
import os

# ====== âœ… å¿½ç•¥ä¸å¿…è¦çš„è­¦å‘Š ======
warnings.filterwarnings("ignore", category=FutureWarning)

# ====== âœ… Flask åˆå§‹åŒ– ======
app = Flask(__name__)
CORS(app)  # â† é€™è¡Œå‹™å¿…åŠ ä¸Šï¼


# ====== âœ… è¨­å®šåƒæ•¸ ======
MILVUS_HOST = "172.23.165.70"
COLLECTION_NAME = "primekg_rag_paths"
EMBED_MODEL = "all-mpnet-base-v2"
#LLM_MODEL = "gemma:2b"
LLM_MODEL = "qwen:0.5b"

# å‘é‡æ¨¡å‹èˆ‡ LLM å»¶é²è¼‰å…¥
embedder = None
llm = None

# ====== âœ… å•Ÿå‹•å‰æª¢æŸ¥ ======
# Milvus å•Ÿå‹•æª¢æŸ¥
def startup_check():
    print("å•Ÿå‹•æª¢æŸ¥ä¸­...")

    try:
        connections.connect("default", host=MILVUS_HOST, port="19530")
        print(f"æˆåŠŸé€£æ¥ Milvus at {MILVUS_HOST}:19530")
    except Exception as e:
        print(f"ç„¡æ³•é€£æ¥ Milvusï¼š{e}")
        sys.exit(1)

    if not utility.has_collection(COLLECTION_NAME):
        print(f"æ‰¾ä¸åˆ° Collectionï¼š{COLLECTION_NAME}")
        sys.exit(1)

    col = Collection(COLLECTION_NAME)
    print(f"æ‰¾åˆ° Collectionï¼š{COLLECTION_NAME}ï¼Œå…± {col.num_entities} ç­†è³‡æ–™")

    print("API æº–å‚™å•Ÿå‹•...\n")

# ====== âœ… ä¸»è·¯ç”±ï¼šRAG æŸ¥è©¢èˆ‡ LLM å›æ‡‰ ======
@app.route("/", methods=["GET"])
def index():
    return "RAG API is running!"


# æ¸¬è©¦ Neo4j æ˜¯å¦æˆåŠŸé€£æ¥ä¸¦èƒ½æŸ¥è³‡æ–™
@app.route("/neo_test", methods=["GET"])
def test_neo4j():
    try:
        conn = Neo4jConnection()
        test_query = "MATCH (n) RETURN COUNT(n) AS count"
        result = conn.query(test_query)
        conn.close()

        node_count = result[0]["count"] if result else 0
        return jsonify({"neo4j_status": "connected", "total_nodes": node_count})
    except Exception as e:
        return jsonify({"neo4j_status": "error", "detail": str(e)}), 500


@app.route("/rag", methods=["POST"])
def rag():
    global embedder, llm

    try:
        user_query = request.json.get("query")
        print(f"[DEBUG] Query received: {user_query}")

        if not user_query:
            return jsonify({"error": "Query not provided"}), 400

        if embedder is None:
            print("[DEBUG] Loading embedder...")
            embedder = SentenceTransformer(EMBED_MODEL)

        query_vector = embedder.encode(user_query).tolist()

        collection = Collection(COLLECTION_NAME)
        collection.load()
        print("[DEBUG] Milvus collection loaded")

        results = collection.search(
            data=[query_vector],
            anns_field="embedding",
            param={"metric_type": "L2", "params": {"nprobe": 10}},
            limit=5,
            output_fields=["source_name", "relation_type", "target_name"]
        )

        context = ""
        for hit in results[0]:
            s = hit.entity.get("source_name")
            r = hit.entity.get("relation_type")
            t = hit.entity.get("target_name")
            context += f"{s} --[{r}]--> {t}\n"

        prompt = f"""ä½ æ˜¯ä¸€ä½çŸ¥è­˜åœ–è­œå°ˆå®¶ï¼Œè«‹æ ¹æ“šä»¥ä¸‹èƒŒæ™¯çŸ¥è­˜å›ç­”ä½¿ç”¨è€…å•é¡Œï¼šèƒŒæ™¯çŸ¥è­˜ï¼š{context}å•é¡Œï¼š{user_query}è«‹ç”¨ç°¡æ˜æ‰¼è¦çš„æ–¹å¼ä½œç­”ã€‚"""

        if llm is None:
            print(f"[DEBUG] Loading LLM: {LLM_MODEL}")
            llm = OllamaLLM(model=LLM_MODEL)

        answer = llm.invoke(prompt)
        print("[DEBUG] LLM å›æ‡‰å®Œæˆ")

        return Response(json.dumps({"answer": answer}, ensure_ascii=False), mimetype="application/json")

    except Exception as e:
        print(f"[ERROR] {str(e)}")  # âœ… å°å‡ºéŒ¯èª¤ç´°ç¯€
        return jsonify({"error": str(e)}), 500

@app.route("/models", methods=["GET"])
def list_models():
    return jsonify({
        "data": [
            {
                "id":LLM_MODEL,
                "object": "model",
                "owned_by": "rag-api",
                "permission": [],
            }
        ],
        "object": "list"
    })

@app.route("/openai/v1/models", methods=["GET"])
def list_models_openai_format():
    return jsonify({
        "object": "list",
        "data": [
            {
                "id": LLM_MODEL,
                "object": "model",
                "owned_by": "rag-api",
                "permission": [],
            }
        ]
    })

'''
@app.route("/chat/completions", methods=["POST"])
def proxy_chat_completions():
    global embedder, llm

    try:
        body = request.get_json(force=True)  # âœ… æ›´ä¿éšªåœ°è§£æ JSON payload
        messages = body.get("messages", [])
        stream = body.get("stream", False)

        user_query = next((m.get("content") for m in reversed(messages) if m.get("role") == "user"), None)
        if not user_query:
            return jsonify({"error": "Query not provided"}), 400

        if embedder is None:
            print("[DEBUG] è¼‰å…¥å‘é‡æ¨¡å‹...")
            embedder = SentenceTransformer(EMBED_MODEL)

        query_vector = embedder.encode(user_query).tolist()

        collection = Collection(COLLECTION_NAME)
        collection.load()

        results = collection.search(
            data=[query_vector],
            anns_field="embedding",
            param={"metric_type": "L2", "params": {"nprobe": 10}},
            limit=5,
            output_fields=["source_name", "relation_type", "target_name"]
        )

        context = ""
        for hit in results[0]:
            s = hit.entity.get("source_name")
            r = hit.entity.get("relation_type")
            t = hit.entity.get("target_name")
            context += f"{s} --[{r}]--> {t}\n"

        prompt = f"""ä½ æ˜¯ä¸€ä½çŸ¥è­˜åœ–è­œå°ˆå®¶ï¼Œè«‹æ ¹æ“šä»¥ä¸‹èƒŒæ™¯çŸ¥è­˜å›ç­”ä½¿ç”¨è€…å•é¡Œï¼šèƒŒæ™¯çŸ¥è­˜ï¼š{context}å•é¡Œï¼š{user_query}è«‹ç”¨ç°¡æ˜æ‰¼è¦çš„æ–¹å¼ä½œç­”ã€‚"""

        if llm is None:
            print("[DEBUG] è¼‰å…¥ LLM æ¨¡å‹...")
            llm = OllamaLLM(model=LLM_MODEL)

        answer = llm.invoke(prompt)

        if stream:
            def generate():
                print("[DEBUG] Streaming å›æ‡‰ä¸­...")
                for chunk in answer.split("\n"):
                    if chunk.strip():
                        yield 'data: ' + json.dumps({
                            "choices": [{
                                "delta": {"content": chunk + "\n"},
                                "index": 0,
                                "finish_reason": None  # âœ… åŠ ä¸Šé€™å€‹æ¬„ä½
                            }],
                            "object": "chat.completion.chunk"
                        }, ensure_ascii=False) + '\n\n'
                yield 'data: ' + json.dumps({
                    "choices": [{
                        "delta": {},
                        "index": 0,
                        "finish_reason": "stop"
                    }],
                    "object": "chat.completion.chunk"
                }) + '\n\n'
                yield 'data: [DONE]\n\n'



            return Response(stream_with_context(generate()), content_type='text/event-stream')

        else:
            return jsonify({
                "id": "chatcmpl-mockid",
                "object": "chat.completion",
                "choices": [
                    {
                        "index": 0,
                        "message": {
                            "role": "assistant",
                            "content": answer
                        },
                        "finish_reason": "stop"
                    }
                ]
            })

    except Exception as e:
        print(f"[ERROR] {str(e)}")
        return jsonify({"error": str(e)}), 500
'''

# ====== âœ… ä¸»è·¯ç”±ï¼šRAG æŸ¥è©¢èˆ‡ LLM å›æ‡‰ ======
@app.route("/openai/chat/completions", methods=["POST", "OPTIONS"])
def proxy_chat_completions():
    global embedder, llm
    if request.method == "OPTIONS":
        return '', 200  # âœ… å›å‚³ 200 è¡¨ç¤ºé æª¢æˆåŠŸ
    try:
        # âœ… å°å‡ºå®Œæ•´ payloadï¼ˆå¯åœ¨ log è¿½è¹¤ï¼‰
        body = request.get_json(force=True)
        print("[DEBUG] æ¥æ”¶åˆ°å®Œæ•´ payload:", body)

        # âœ… WebUI å¸¸å‚³ä¾†å†—é¤˜æ¬„ä½ï¼Œé€™é‚Šéæ¿¾åƒ…å–å¿…è¦æ¬„ä½
        messages = body.get("messages", [])
        if not messages or not isinstance(messages, list):
            return jsonify({"error": "messages æ¬„ä½æ ¼å¼éŒ¯èª¤"}), 400

        # âœ… å¼·åˆ¶é—œé–‰ stream æ¨¡å¼é€²è¡Œ debugï¼ˆå…ˆè·‘é€šåŸºæœ¬æµç¨‹ï¼‰
        stream = False

        # âœ… å–å¾—æœ€æ–°ä¸€ç­† user æŸ¥è©¢
        user_query = next((m.get("content") for m in reversed(messages) if m.get("role") == "user"), None)
        if not user_query:
            return jsonify({"error": "æ‰¾ä¸åˆ° user æå•å…§å®¹"}), 400

        print(f"[DEBUG] ä½¿ç”¨è€…æŸ¥è©¢å…§å®¹: {user_query}")

        # âœ… è¼‰å…¥å‘é‡æ¨¡å‹
        if embedder is None:
            print("[DEBUG] è¼‰å…¥å‘é‡æ¨¡å‹ä¸­...")
            embedder = SentenceTransformer(EMBED_MODEL)

        # âœ… æŸ¥è©¢ Neo4j ä¸¦è½‰æ›ç‚ºå‘é‡
        print("[DEBUG] æŸ¥è©¢ Neo4j ä¸¦è½‰æ›ç‚ºå‘é‡...")
        conn = Neo4jConnection()
        neo4j_query = """
        MATCH path = (n)-[:bioprocess_protein|pathway_protein|disease_protein|drug_effect|indication|phenotype_protein|drug_protein]-(m)
        WHERE n.node_name CONTAINS "stem cell" OR n.node_name CONTAINS "regenerative medicine"
        RETURN path
        """
        data = conn.query(neo4j_query)
        conn.close()
        print("ğŸ”Œ Neo4j é€£ç·šå·²é—œé–‰")

        # âœ… æŸ¥è©¢çµæœéæ¿¾æˆä¸Šä¸‹æ–‡é—œä¿‚åœ–
        query_vector = embedder.encode(user_query).tolist()
        results = []
        seen = set()
        for i, item in enumerate(data):
            path = item.get("path", [])
            if len(path) != 3:
                continue
            source_node, relation_type, target_node = path
            key = (source_node["node_name"], relation_type, target_node["node_name"])
            if key in seen:
                continue
            seen.add(key)
            results.append(key)

        # âœ… çµ„åˆèƒŒæ™¯çŸ¥è­˜ context
        context = "\n".join([f"{s} --[{r}]--> {t}" for s, r, t in results[:5]])
        prompt = f"""ä½ æ˜¯ä¸€ä½çŸ¥è­˜åœ–è­œå°ˆå®¶ï¼Œè«‹æ ¹æ“šä»¥ä¸‹èƒŒæ™¯çŸ¥è­˜å›ç­”ä½¿ç”¨è€…å•é¡Œï¼šèƒŒæ™¯çŸ¥è­˜ï¼š{context}å•é¡Œï¼š{user_query}è«‹ç”¨ç°¡æ˜æ‰¼è¦çš„æ–¹å¼ä½œç­”ã€‚"""

        # âœ… è¼‰å…¥ LLM ä¸¦æ¨ç†
        if llm is None:
            print("[DEBUG] è¼‰å…¥ LLM æ¨¡å‹ä¸­...")
            llm = OllamaLLM(model=LLM_MODEL)

        print("[DEBUG] å‘¼å« LLM ç”Ÿæˆå›ç­”ä¸­...")
        answer = llm.invoke(prompt)

        # âœ… é streaming æ¨¡å¼ä¸‹å›å‚³æ¨™æº–æ ¼å¼
        return jsonify({
            "id": "chatcmpl-mockid",
            "object": "chat.completion",
            "created": int(time.time()),               # âœ… åŠ ä¸Š created timestamp
            "model": body.get("model", "unknown"),     # âœ… åŠ ä¸Š model åç¨±
            "choices": [
                {
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": answer
                    },
                    "finish_reason": "stop"
                }
            ],
            "usage": {                                 # âœ… åŠ ä¸Š token ä½¿ç”¨æƒ…æ³ï¼ˆæš«å¡«å¯«ï¼‰
                "prompt_tokens": 0,
                "completion_tokens": 0,
                "total_tokens": 0
            }
        })

    except Exception as e:
        print(f"[âŒ ERROR] ChatCompletions å…§éƒ¨éŒ¯èª¤ï¼š{str(e)}")
        return jsonify({"error": str(e)}), 500


# åˆ—å‡ºå¯ç”¨ç¶²å€ï¼ˆé¡¯ç¤º IP ï¼‰
def show_access_urls():
    hostname = socket.gethostname()
    local_ip = socket.gethostbyname(hostname)
    print("é€éä»¥ä¸‹ç¶²å€è¨ªå• APIï¼š")
    print(f" â†’ æœ¬æ©Ÿæ¸¬è©¦ç”¨ï¼š http://127.0.0.1:5000")
    print(f" â†’ å€ç¶²è¨­å‚™ç”¨ï¼š http://{local_ip}:5000")


# ====== âœ… å•Ÿå‹• Flask ä¼ºæœå™¨ ======
if __name__ == "__main__":
    startup_check()
    show_access_urls()

    # å¯«å…¥ open-webui/.flask_ip
    flask_ip_path = os.path.join(os.path.dirname(__file__), "open-webui", ".flask_ip")
    local_ip = socket.gethostbyname(socket.gethostname())
    with open(flask_ip_path, "w") as f:
        f.write(local_ip)

    app.run(host="0.0.0.0", port=5000)
