import time
import json
import warnings
import sys
import socket
import threading
from flask import Flask, request, jsonify
from flask_cors import CORS
from pymilvus import connections, Collection, utility
from sentence_transformers import SentenceTransformer
from langchain_ollama import OllamaLLM
from backend.neo4j_connect import Neo4jConnection
import os
import numpy as np

# ====== âœ… å¿½ç•¥ä¸å¿…è¦çš„è­¦å‘Š ======
warnings.filterwarnings("ignore", category=FutureWarning)

# ====== âœ… Flask åˆå§‹åŒ– ======
app = Flask(__name__)
CORS(app)

# ====== âœ… è¨­å®šåƒæ•¸ ======
MILVUS_HOST = "127.0.0.1"
COLLECTION_NAME = "collection_text"
COLLECTION_KGE = "collection_kge" 
EMBED_MODEL = "all-mpnet-base-v2"
DEFAULT_LLM_MODEL = "deepseek-r1:1.5b"  
DATA_DIR = "data"

# ====== âœ… å…¨åŸŸè®Šæ•¸ï¼šå»¶é²è¼‰å…¥ Embedding èˆ‡ LLM ======
embedder = None
llm_cache = {}
load_lock = threading.Lock()  # åˆå§‹åŒ–é–ï¼Œé¿å… race condition

# ====== âœ… è¼‰å…¥ entity_to_vec (KGE) ======
print("ğŸ”„ è¼‰å…¥ entity_to_vec...")
name_map = {}
with open(os.path.join(DATA_DIR, "entity_name_map.tsv"), "r", encoding="utf-8") as f:
    for line in f:
        safe_id, original_name = line.strip().split("\t")
        name_map[safe_id] = original_name

entity_embeddings = np.load(os.path.join(DATA_DIR, "entity_embeddings.npy"))
with open(os.path.join(DATA_DIR, "entity_names.txt"), "r", encoding="utf-8") as f:
    safe_ids = [line.strip() for line in f if line.strip()]

entity_to_vec = {
    name_map[safe_id]: vec
    for safe_id, vec in zip(safe_ids, entity_embeddings)
    if safe_id in name_map
}
print(f"âœ… å·²è¼‰å…¥ entity_to_vecï¼Œå…± {len(entity_to_vec)} ç­†å¯¦é«”")


# ====== âœ… å•Ÿå‹•æª¢æŸ¥ ======
def startup_check():
    print("å•Ÿå‹•æª¢æŸ¥ä¸­...")
    try:
        connections.connect("default", host=MILVUS_HOST, port="19530")
        print(f"âœ… æˆåŠŸé€£æ¥ Milvus at {MILVUS_HOST}:19530")
    except Exception as e:
        print(f"âŒ ç„¡æ³•é€£æ¥ Milvusï¼š{e}")
        sys.exit(1)

    for col_name in [COLLECTION_NAME, COLLECTION_KGE]:
        if not utility.has_collection(col_name):
            print(f"âŒ æ‰¾ä¸åˆ° Collectionï¼š{col_name}")
            sys.exit(1)
        col = Collection(col_name)
        print(f"âœ… æ‰¾åˆ° Collectionï¼š{col_name}ï¼Œå…± {col.num_entities} ç­†è³‡æ–™")

    print("API æº–å‚™å•Ÿå‹•...\n")

# ====== âœ… è·¯ç”±å€ ======
@app.route("/", methods=["GET"])
def index():
    return "âœ… RAG API is running!"

@app.route("/status", methods=["GET"])
def status():
    return jsonify({
        "embedder_loaded": embedder is not None,
        "llm_loaded": len(llm_cache) > 0,
        "kge_loaded": len(entity_to_vec) > 0
    })

@app.route("/neo_test", methods=["GET"])
def test_neo4j():
    try:
        conn = Neo4jConnection()
        result = conn.query("MATCH (n) RETURN COUNT(n) AS count")
        conn.close()
        count = result[0]["count"] if result else 0
        return jsonify({"neo4j_status": "connected", "total_nodes": count})
    except Exception as e:
        return jsonify({"neo4j_status": "error", "detail": str(e)}), 500

@app.route("/rag", methods=["POST"])
def rag():
    global embedder

    try:
        user_query = request.json.get("query")
        mode = request.json.get("mode", "åŒæ™‚é¡¯ç¤ºå…©è€…")  # é è¨­ç‚ºé¡¯ç¤ºå…©è€…
        model_name = request.json.get("model", DEFAULT_LLM_MODEL)

        print(f"[DEBUG] Query received: {user_query}")
        print(f"[DEBUG] Mode received: {mode}")
        print(f"[DEBUG] Model selected: {model_name}")

        if not user_query:
            return jsonify({"error": "Query not provided"}), 400

        answer_rag = answer_no_rag = None

        # ====== è¼‰å…¥ LLM æ¨¡å‹ ======
        if model_name not in llm_cache:
            with load_lock:
                if model_name not in llm_cache:
                    print(f"[DEBUG] è¼‰å…¥ LLM æ¨¡å‹ï¼š{model_name}")
                    llm_cache[model_name] = OllamaLLM(model=model_name)
        llm = llm_cache[model_name]

        

        # ====== RAG å›æ‡‰æµç¨‹ ======
        if mode in ["RAG å›æ‡‰", "åŒæ™‚é¡¯ç¤ºå…©è€…"]:
            try:
                # è¼‰å…¥ embedder
                if embedder is None:
                    with load_lock:
                        if embedder is None:
                            print("[DEBUG] Loading embedder...")
                            embedder = SentenceTransformer(EMBED_MODEL)
                            print("[DEBUG] Embedder loaded")

                # å‘é‡åŒ–
                query_vector = embedder.encode(user_query).tolist()
                print(f"[DEBUG] Query vector generated. Length: {len(query_vector)}")

                # === Step 1: Text æª¢ç´¢ï¼ŒæŸ¥è©¢ Milvus ===
                collection = Collection(COLLECTION_NAME)
                collection.load()
                print("[DEBUG] Milvus collection loaded")
                results = collection.search(
                    data=[query_vector],
                    anns_field="embedding",
                    param={"metric_type": "L2", "params": {"nprobe": 10}},
                    limit=5,
                    output_fields=["source_name", "relation_type", "target_name"]
                )
                print(f"[DEBUG] Milvus search returned {len(results[0]) if results else 0} results")

                # å»ºç«‹ context
                context = ""
                related_entities = set()

                if results and results[0]:
                    for hit in results[0]:
                        s = hit.entity.get("source_name")
                        r = hit.entity.get("relation_type")
                        t = hit.entity.get("target_name")
                        context += f"{s} --[{r}]--> {t}\n"
                        related_entities.update([s, t])
                else:
                    print("[DEBUG] No context found from Milvus(Text)")

                


                # === Step 2: KGE æª¢ç´¢ ===
                if related_entities and entity_to_vec:
                    print(f"[DEBUG] Doing KGE search for {len(related_entities)} entities...")
                    matched_vecs = [entity_to_vec[e] for e in related_entities if e in entity_to_vec]

                    # fallback
                    if not matched_vecs:
                        matched_vecs = [np.zeros(entity_embeddings.shape[1]).tolist()]

                    collection_kge = Collection(COLLECTION_KGE)
                    collection_kge.load()
                    kge_results = collection_kge.search(
                        data=matched_vecs,
                        anns_field="embedding",
                        param={"metric_type": "L2", "params": {"nprobe": 10}},
                        limit=3,
                        output_fields=["entity_name"]
                    )
                    for group in kge_results:
                        for r in group:
                            context += f"KGE Suggestion Entity: {r.entity.get('entity_name')}\n"
                            
                print(f"[DEBUG] Final context:\n{context if context else '[ç©ºç™½]'}")

                # å»ºç«‹ prompt + å›æ‡‰
                if context.strip():
                    # prompt_rag = f"""
                    # ä½ æ˜¯ä¸€ä½çŸ¥è­˜åœ–è­œå°ˆå®¶ã€‚
                    # ä»¥ä¸‹åŒ…å«å…©éƒ¨åˆ†æª¢ç´¢çµæœï¼š
                    #     1. Text-based retrieval (Neo4j ä¸‰å…ƒçµ„)
                    #     2. KGE-based retrieval (çµæ§‹ç›¸ä¼¼å¯¦é«”)

                    # èƒŒæ™¯çŸ¥è­˜ï¼š{context}
                    # å•é¡Œï¼š{user_query}
                    # è«‹ç”¨ç°¡æ˜æ‰¼è¦çš„æ–¹å¼ä½œç­”ã€‚
                    # """

                    # prompt_rag = f"""
                    #     You are a biomedical knowledge graph expert.

                    #     Background knowledge:
                    #     {context}

                    #     Question: {user_query}

                    #     ### Instructions:
                    #     - Answer ONLY with the exact entity names from the knowledge graph (English).
                    #     - Provide them as a comma-separated list.
                    #     - Do NOT translate into Chinese.
                    #     - Do NOT add explanations or numbers.
                    #     """
                    prompt_rag = f"""
                    You are a biomedical knowledge graph expert.

                    Background knowledge:
                    {context}

                    Question: {user_query}

                    ### Instructions:
                    - Extract ONLY gene names from the background knowledge above.
                    - Do NOT output drugs or diseases.
                    - Respond ONLY in valid JSON, nothing else.

                    Output format:
                    {{
                    "genes": ["CYP3A4", "DRD2", "GRIN2B"]
                    }}
                    """
                    


                    print("[DEBUG] ç”Ÿæˆ RAG å›æ‡‰ä¸­...")
                    answer_rag = llm.invoke(prompt_rag)
                    # print("[DEBUG] LLM Answer (RAG):", answer_rag)
                    if not answer_rag.strip():
                        print("[WARNING] RAG å›æ‡‰ç‚ºç©ºå­—ä¸²")
                else:
                    answer_rag = "(æŸ¥ç„¡ç›¸é—œçŸ¥è­˜ï¼Œç„¡æ³•ä½¿ç”¨ RAG å›æ‡‰)"
            except Exception as e:
                print(f"[ERROR] RAG å›æ‡‰å¤±æ•—ï¼š{e}")
                answer_rag = f"(RAG å›æ‡‰éŒ¯èª¤ï¼š{e})"

        # ====== no-RAG å›æ‡‰æµç¨‹ ======
        if mode in ["ä¸ä½¿ç”¨ RAG å›æ‡‰", "åŒæ™‚é¡¯ç¤ºå…©è€…"]:
            try:
                prompt_no_rag = f"ä½ æ˜¯ä¸€ä½çŸ¥è­˜åœ–è­œå°ˆå®¶ï¼Œè«‹æ ¹æ“šä½ è‡ªå·±çš„çŸ¥è­˜å›ç­”ä»¥ä¸‹å•é¡Œï¼š{user_query}ã€‚è«‹ç”¨ç°¡æ˜æ‰¼è¦çš„æ–¹å¼ä½œç­”ã€‚"
                print("[DEBUG] ç”Ÿæˆ No-RAG å›æ‡‰ä¸­...")
                answer_no_rag = llm.invoke(prompt_no_rag)
                print("[DEBUG] LLM Answer (No-RAG):", answer_no_rag)
                if not answer_no_rag.strip():
                    print("[WARNING] No-RAG å›æ‡‰ç‚ºç©ºå­—ä¸²")
            except Exception as e:
                print(f"[ERROR] No-RAG å›æ‡‰å¤±æ•—ï¼š{e}")
                answer_no_rag = f"(No-RAG å›æ‡‰éŒ¯èª¤ï¼š{e})"

        return jsonify({
            "model": model_name,
            "mode": mode,
            "query": user_query,
            "context": context if "context" in locals() else None,
            "prompt_rag": prompt_rag if "prompt_rag" in locals() else None,
            "milvus_hits": [
                {
                    "source": hit.entity.get("source_name"),
                    "relation": hit.entity.get("relation_type"),
                    "target": hit.entity.get("target_name"),
                    "score": hit.distance
                }
                for hit in results[0]
            ] if results and results[0] else [],
            "answer_rag": answer_rag,
            "answer_no_rag": answer_no_rag
        })

    except Exception as e:
        print(f"[FATAL ERROR] {str(e)}")
        return jsonify({"error": str(e)}), 500

# ====== âœ… é¡¯ç¤ºç¶²å€æç¤º ======
def show_access_urls():
    hostname = socket.gethostname()
    local_ip = socket.gethostbyname(hostname)
    print("ğŸ“¡ é€éä»¥ä¸‹ç¶²å€è¨ªå• APIï¼š")
    print(f" â†’ æœ¬æ©Ÿæ¸¬è©¦ç”¨ï¼šhttp://127.0.0.1:5000")
    print(f" â†’ å€ç¶²è¨­å‚™ç”¨ï¼šhttp://{local_ip}:5000")

# ====== âœ… å•Ÿå‹• Flask ======
if __name__ == "__main__":
    startup_check()
    show_access_urls()
    app.run(host="0.0.0.0", port=5000)
