# query_main.py å¼·åŒ–ç‰ˆï¼Œæ’å…¥å±¬æ€§æ¨è«–æ¨¡çµ„
import sys
import os
import csv
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from backend.milvus_connection import connect_milvus
from pymilvus import Collection
from sentence_transformers import SentenceTransformer
from langchain_ollama import OllamaLLM
import json
import numpy as np

from attribute_pipeline.attribute_reasoner import infer_attributes_from_context as infer_attributes # âœ… æ–°å¢

def load_entity_attributes_csv(file_path):
    entity_attr_map = {}
    with open(file_path, "r", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            entity = row["entity_id"]
            attributes = []
            for attr, value in row.items():
                if attr == "entity_id":
                    continue
                if value == "1":
                    attributes.append(attr)
            entity_attr_map[entity] = attributes
    return entity_attr_map

# === âœ… è¼‰å…¥ sanitized â†’ åŸå§‹åç¨±å°æ‡‰ ===
DATA_DIR = "data"
name_map = {}
with open(os.path.join(DATA_DIR, "entity_name_map.tsv"), "r", encoding="utf-8") as f:
    for line in f:
        safe_id, original_name = line.strip().split("\t")
        name_map[safe_id] = original_name

# === è¼‰å…¥ entity å‘é‡èˆ‡ safe_idï¼ˆèˆ‡ name_map æ­é…ä½¿ç”¨ï¼‰ ===
entity_embeddings = np.load(os.path.join(DATA_DIR, "entity_embeddings.npy"))
with open(os.path.join(DATA_DIR, "entity_names.txt"), "r", encoding="utf-8") as f:
    safe_ids = [line.strip() for line in f if line.strip()]

entity_to_vec = {
    name_map[safe_id]: vec
    for safe_id, vec in zip(safe_ids, entity_embeddings)
    if safe_id in name_map
}

# === è¼‰å…¥å¯¦é«”å±¬æ€§è¡¨æ ¼ï¼ˆåŸå§‹ entity_attributes.csvï¼‰ ===
entity_attr_map = load_entity_attributes_csv(os.path.join(DATA_DIR, "entity_attributes.csv"))

# === è¼‰å…¥ PAG é—œè¯åœ– ===
with open(os.path.join(DATA_DIR, "attribute_graph.json"), "r", encoding="utf-8") as f:
    attribute_graph = json.load(f)


def prompt_find_connections(context: str, query: str) -> str:
    return f"""æ ¹æ“šä»¥ä¸‹çŸ¥è­˜åœ–è­œè³‡æ–™èˆ‡å±¬æ€§é‚è¼¯ï¼Œæ¢ç´¢èˆ‡ã€Œ{query}ã€å¯èƒ½æœ‰æ½›åœ¨é—œè¯çš„åŸºå› ã€è—¥ç‰©æˆ–ç”Ÿç‰©éç¨‹ã€‚
ã€è³‡æ–™ã€‘{context} è«‹ç”¨ä¸­æ–‡æ¢åˆ—åˆ—å‡ºé—œè¯å¯¦é«”ï¼Œä¸¦ç°¡è¦èªªæ˜æ¨æ¸¬çš„åŸå› æˆ–é—œä¿‚é‚è¼¯ã€‚"""


def hybrid_retrieval_and_llm(query, text_model, kge_collection, text_collection):
    query_text = text_model.encode(query).tolist()
    search_param = {"metric_type": "L2", "params": {"nprobe": 10}}

    text_results = text_collection.search(
        data=[query_text],
        anns_field="embedding",
        param=search_param,
        limit=3,
        output_fields=["source_name", "relation_type", "target_name"]
    )

    related_entities = set()
    for r in text_results[0]:
        related_entities.add(r.entity.get("source_name"))
        related_entities.add(r.entity.get("target_name"))

    matched_vecs = [entity_to_vec[name] for name in related_entities if name in entity_to_vec]
    if not matched_vecs:
        matched_vecs = [np.zeros(50).tolist()]

    kge_results = kge_collection.search(
        data=matched_vecs,
        anns_field="embedding",
        param=search_param,
        limit=3,
        output_fields=["entity_name"]
    )

    # === ğŸ§  æ–°å¢å±¬æ€§æ¨è«–éƒ¨åˆ† ===
    reasoning_output = ""
    for entity in related_entities:
        if entity in entity_attr_map:
            known_attrs = entity_attr_map[entity]
            inferred_attrs = infer_attributes(known_attrs, attribute_graph)
            reasoning_output += f"\nEntity: {entity}\nKnown: {known_attrs}\nInferred: {inferred_attrs}\n"

    # === æ‹¼æ¥ context çµ¦ LLM ===
    context = ""
    for r in text_results[0]:
        context += f"{r.entity.get('source_name')} -[{r.entity.get('relation_type')}]â†’ {r.entity.get('target_name')}\n"
    for group in kge_results:
        for r in group:
            context += f"KGE Suggestion Entity: {r.entity.get('entity_name')}\n"

    # âœ… æ’å…¥å±¬æ€§æ¨è«–è£œå……
    context += f"\n[å±¬æ€§é‚è¼¯æ¨è«–]\n{reasoning_output}"

    prompt = prompt_find_connections(context, query)
    #llm = OllamaLLM(model="gemma:2b")
    llm = OllamaLLM(base_url="http://127.0.0.1:11435", model="gemma:2b")

    response = llm.invoke(f"{context}\n\n{prompt}")
    print("\nå›ç­”å…§å®¹ï¼š\n")
    print(response)


if __name__ == "__main__":
    print("åˆå§‹åŒ–æ¨¡å‹èˆ‡ Milvus")
    connect_milvus()
    text_model = SentenceTransformer("all-mpnet-base-v2")
    collection_text = Collection("collection_text")
    collection_kge = Collection("collection_kge")
    collection_text.load()
    collection_kge.load()

    query = input("è«‹è¼¸å…¥æŸ¥è©¢å•é¡Œï¼ˆå¦‚ï¼šstem cell regeneration in mammary glandï¼‰: ")
    hybrid_retrieval_and_llm(query, text_model, collection_kge, collection_text)
